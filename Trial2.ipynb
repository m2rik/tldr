{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 857,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 229298,
     "status": "ok",
     "timestamp": 1524566989869,
     "user": {
      "displayName": "Varun Vora",
      "photoUrl": "//lh6.googleusercontent.com/-1IwMwD9ergk/AAAAAAAAAAI/AAAAAAAAAhM/n6PFILx4QTg/s50-c-k-no/photo.jpg",
      "userId": "101932039486808701516"
     },
     "user_tz": -330
    },
    "id": "p4OyyP9zcoW2",
    "outputId": "30233311-fe5f-4177-9d1a-00f1c3a41e87"
   },
   "outputs": [],
   "source": [
    "# # run this cell every time you restart\n",
    "# from google.colab import files\n",
    "\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# for fn in uploaded.keys():\n",
    "#   print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "#       name=fn, length=len(uploaded[fn])))\n",
    "# !pip install gensim\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "iqLctjfqLlwl"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import TimeDistributed, RepeatVector, LSTM, Dense, Embedding, Input, concatenate\n",
    "from keras.preprocessing.text import Tokenizer, one_hot, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec\n",
    "from dataset_loader import load_dataset\n",
    "from collections import Counter\n",
    "from random import randrange, random\n",
    "from string import punctuation\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from pickle import load\n",
    "from copy import deepcopy as dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 923,
     "status": "ok",
     "timestamp": 1524567070125,
     "user": {
      "displayName": "Varun Vora",
      "photoUrl": "//lh6.googleusercontent.com/-1IwMwD9ergk/AAAAAAAAAAI/AAAAAAAAAhM/n6PFILx4QTg/s50-c-k-no/photo.jpg",
      "userId": "101932039486808701516"
     },
     "user_tz": -330
    },
    "id": "90BdLcj-QtWV",
    "outputId": "c8b893dc-d667-41ef-efd9-f6e3ce031701"
   },
   "outputs": [],
   "source": [
    "x, y = load_dataset()\n",
    "with open(\"fixed_len_dataset.pkl\", \"rb\") as fp:\n",
    "  x = load(fp)\n",
    "\n",
    "x, y = zip(*list(filter(lambda x : len(text_to_word_sequence(x[1])) == 60, zip(x,y)))) #Selects only those data points whose summary length is 60 words\n",
    "\n",
    "all_words = \" \".join(x) + \" \".join(y)\n",
    "all_words = text_to_word_sequence(all_words)\n",
    "\n",
    "print(x[0])\n",
    "print(y[0])\n",
    "\n",
    "vocab_size = len(set(all_words)) #56558, 27178 for only summaries with 60 words\n",
    "\n",
    "print(\"Size of vocabulary : \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2882,
     "status": "ok",
     "timestamp": 1524567085376,
     "user": {
      "displayName": "Varun Vora",
      "photoUrl": "//lh6.googleusercontent.com/-1IwMwD9ergk/AAAAAAAAAAI/AAAAAAAAAhM/n6PFILx4QTg/s50-c-k-no/photo.jpg",
      "userId": "101932039486808701516"
     },
     "user_tz": -330
    },
    "id": "t3CzudAAmNPV",
    "outputId": "4fe0feaa-4afa-4849-eacf-e4b1f7ebb9df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424\n",
      "['The', 'Daman', 'and', 'Diu', 'administration', 'on', 'Wednesday', 'withdrew', 'a', 'circular', 'that', 'asked', 'women', 'staff', 'to', 'tie', 'rakhis', 'on', 'male', 'colleagues', 'after', 'the', 'order', 'triggered', 'a', 'backlash', 'from', 'employees', 'and', 'was', 'ripped', 'apart', 'on', 'social', 'media.The', 'union', 'territory?s', 'administration', 'was', 'forced', 'to', 'retreat', 'within', '24', 'hours', 'of', 'issuing', 'the', 'circular', 'that', 'made', 'it', 'compulsory', 'for', 'its', 'staff', 'to', 'celebrate', 'Rakshabandhan', 'at', 'workplace.?It', 'has', 'been', 'decided', 'to', 'celebrate', 'the', 'festival', 'of', 'Rakshabandhan', 'on', 'August', '7.', 'In', 'this', 'connection,', 'all', 'offices/', 'departments', 'shall', 'remain', 'open', 'and', 'celebrate', 'the', 'festival', 'collectively', 'at', 'a', 'suitable', 'time', 'wherein', 'all', 'the', 'lady', 'staff', 'shall', 'tie', 'rakhis', 'to', 'their', 'colleagues,?', 'the', 'order,', 'issued', 'on', 'August', '1', 'by', 'Gurpreet', 'Singh,', 'deputy', 'secretary', '(personnel),', 'had', 'said.To', 'ensure', 'that', 'no', 'one', 'skipped', 'office,', 'an', 'attendance', 'report', 'was', 'to', 'be', 'sent', 'to', 'the', 'government', 'the', 'next', 'evening.The', 'two', 'notifications', '?', 'one', 'mandating', 'the', 'celebration', 'of', 'Rakshabandhan', '(left)', 'and', 'the', 'other', 'withdrawing', 'the', 'mandate', '(right)', '?', 'were', 'issued', 'by', 'the', 'Daman', 'and', 'Diu', 'administration', 'a', 'day', 'apart.', 'The', 'circular', 'was', 'withdrawn', 'through', 'a', 'one-line', 'order', 'issued', 'late', 'in', 'the', 'evening', 'by', 'the', 'UT?s', 'department', 'of', 'personnel', 'and', 'administrative', 'reforms.?The', 'circular', 'is', 'ridiculous.', 'There', 'are', 'sensitivities', 'involved.', 'How', 'can', 'the', 'government', 'dictate', 'who', 'I', 'should', 'tie', 'rakhi', 'to?', 'We', 'should', 'maintain', 'the', 'professionalism', 'of', 'a', 'workplace?', 'an', 'official', 'told', 'Hindustan', 'Times', 'earlier', 'in', 'the', 'day.', 'She', 'refused', 'to', 'be', 'identified.The', 'notice', 'was', 'issued', 'on', 'Daman', 'and', 'Diu', 'administrator', 'and', 'former', 'Gujarat', 'home', 'minister', 'Praful', 'Kodabhai', 'Patel?s', 'direction,', 'sources', 'said.Rakshabandhan,', 'a', 'celebration', 'of', 'the', 'bond', 'between', 'brothers', 'and', 'sisters,', 'is', 'one', 'of', 'several', 'Hindu', 'festivities', 'and', 'rituals', 'that', 'are', 'no', 'longer', 'confined', 'of', 'private,', 'family', 'affairs', 'but', 'have', 'become', 'tools', 'to', 'push', 'politic', 'al', 'ideologies.In', '2014,', 'the', 'year', 'BJP', 'stormed', 'to', 'power', 'at', 'the', 'Centre,', 'Rashtriya', 'Swayamsevak', 'Sangh', '(RSS)', 'chief', 'Mohan', 'Bhagwat', 'said', 'the', 'festival', 'had', '?national', 'significance?', 'and', 'should', 'be', 'celebrated', 'widely', '?to', 'protect', 'Hindu', 'culture', 'and', 'live', 'by', 'the', 'values', 'enshrined', 'in', 'it?.', 'The', 'RSS', 'is', 'the', 'ideological', 'parent', 'of', 'the', 'ruling', 'BJP.Last', 'year,', 'women', 'ministers', 'in', 'the', 'Modi', 'government', 'went', 'to', 'the', 'border', 'areas', 'to', 'celebrate', 'the', 'festival', 'with', 'soldiers.', 'A', 'year', 'before,', 'all', 'cabinet', 'ministers', 'were', 'asked', 'to', 'go', 'to', 'their', 'constituencies', 'for', 'the', 'festival.', 'The', 'Administration', 'of', 'Union', 'Territory', 'Daman', 'and', 'Diu', 'has', 'revoked', 'its', 'order', 'that', 'made', 'it', 'compulsory', 'for', 'women', 'to', 'tie', 'rakhis', 'to', 'their', 'male', 'colleagues', 'on', 'the', 'occasion', 'of', 'Rakshabandhan', 'on', 'August', '7.', 'The', 'administration', 'was', 'forced', 'to', 'withdraw', 'the', 'decision', 'within', '24', 'hours', 'of', 'issuing', 'the', 'circular', 'after', 'it', 'received', 'flak', 'from', 'employees', 'and', 'was', 'slammed', 'on', 'social', 'media.']\n",
      "24245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Varun\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\Varun\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4396,)\n",
      "(4396, 60, 100)\n"
     ]
    }
   ],
   "source": [
    "# tokenized x, y and xy\n",
    "tok_x = [xi.split() for xi in x]\n",
    "tok_y = [yi.split()[:60] + max(0, 60 - len(yi.split())) * [\"__unk__\"] for yi in y]\n",
    "tok_xy = [xi + yi for xi, yi in zip(tok_x, tok_y)]\n",
    "\n",
    "# len_tok_x = set([len(xi) for xi in tok_x])\n",
    "# print(len_tok_x)\n",
    "\n",
    "# len_tok_y = set([len(yi) for yi in tok_y])\n",
    "# print(len_tok_y)\n",
    "\n",
    "print(len(tok_xy[0]))\n",
    "print(tok_xy[0])\n",
    "\n",
    "tok_xy.append(5 * [\"__unk__\"])\n",
    "\n",
    "# word2vec model\n",
    "model_w2v = Word2Vec(tok_xy, size = 100, min_count = 5)\n",
    "\n",
    "# size of vocab\n",
    "words = list(model_w2v.wv.vocab)\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)\n",
    "\n",
    "# replacing unknown words\n",
    "tok_x = [[word if word in model_w2v.wv.vocab else \"__unk__\" for word in xi] for xi in tok_x]\n",
    "tok_y = [[word if word in model_w2v.wv.vocab else \"__unk__\" for word in yi] for yi in tok_y]\n",
    "\n",
    "# x values\n",
    "x_train_article = np.array([[model_w2v[word] for word in xi] for xi in tok_x])\n",
    "x_train_summary = np.array([[model_w2v[word] for word in yi] for yi in tok_y])\n",
    "\n",
    "print(x_train_article.shape)\n",
    "print(x_train_summary.shape)\n",
    "\n",
    "del tok_x\n",
    "del tok_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6612,
     "status": "ok",
     "timestamp": 1524568811113,
     "user": {
      "displayName": "Varun Vora",
      "photoUrl": "//lh6.googleusercontent.com/-1IwMwD9ergk/AAAAAAAAAAI/AAAAAAAAAhM/n6PFILx4QTg/s50-c-k-no/photo.jpg",
      "userId": "101932039486808701516"
     },
     "user_tz": -330
    },
    "id": "M9dSnpTJQXog",
    "outputId": "c1e42114-baee-43ad-bff0-e7adf4ef1446"
   },
   "outputs": [],
   "source": [
    "print(words[:10])\n",
    "\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(words)\n",
    "\n",
    "print(label_encoder.transform([\"world\"]))\n",
    "\n",
    "# one-hot encode\n",
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(range(vocab_size))\n",
    "\n",
    "y_train = np.array([label_binarizer.transform(label_encoder.transform(yi)) for yi in tok_y])\n",
    "print(y_train.shape)\n",
    "\n",
    "# required shape -> 1112, 60, vocab_size\n",
    "\n",
    "# invert first example\n",
    "inverted = label_encoder.inverse_transform([argmax(onehot_encoded[0, :])])\n",
    "print(inverted)\n",
    "#print(words[inverted])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 234661,
     "status": "ok",
     "timestamp": 1524567333509,
     "user": {
      "displayName": "Varun Vora",
      "photoUrl": "//lh6.googleusercontent.com/-1IwMwD9ergk/AAAAAAAAAAI/AAAAAAAAAhM/n6PFILx4QTg/s50-c-k-no/photo.jpg",
      "userId": "101932039486808701516"
     },
     "user_tz": -330
    },
    "id": "rXkz1w-z7_6u",
    "outputId": "2562c112-f138-4b8c-a582-825397d4917e"
   },
   "outputs": [],
   "source": [
    "# Source:  Alternate 3 from https://machinelearningmastery.com/encoder-decoder-models-text-summarization-keras/\n",
    "epochs = 16\n",
    "batch_size = 32\n",
    "\n",
    "src_txt_length = 100\n",
    "sum_txt_length = 60\n",
    "word_vec_length = 100\n",
    "\n",
    "# article input model\n",
    "inputs1 = Input(shape=(src_txt_length, word_vec_length))\n",
    "# article1 = Embedding(vocab_size, 128)(inputs1)\n",
    "article2 = LSTM(128)(inputs1)\n",
    "article3 = RepeatVector(sum_txt_length)(article2)\n",
    "\n",
    "# summary input model\n",
    "inputs2 = Input(shape=(sum_txt_length, word_vec_length))\n",
    "# summ1 = Embedding(vocab_size, 128)(inputs2)\n",
    "\n",
    "# decoder model\n",
    "decoder1 = concatenate([article3, inputs2])\n",
    "decoder2 = LSTM(128, return_sequences = True)(decoder1)                           # made changes here\n",
    "outputs = TimeDistributed(Dense(vocab_size, activation='softmax'))(decoder2)      # made changes here\n",
    "\n",
    "# tie it together [article, summary] [word]\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['acc'])\n",
    "model.fit([x_train_article, x_train_summary], y_train, epochs = epochs, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1319
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1654,
     "status": "error",
     "timestamp": 1524567341720,
     "user": {
      "displayName": "Varun Vora",
      "photoUrl": "//lh6.googleusercontent.com/-1IwMwD9ergk/AAAAAAAAAAI/AAAAAAAAAhM/n6PFILx4QTg/s50-c-k-no/photo.jpg",
      "userId": "101932039486808701516"
     },
     "user_tz": -330
    },
    "id": "cP3w_LOsJPwz",
    "outputId": "376d1813-53f0-480a-d673-f3609880ce93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Va:  4341\n",
      "Vs:  4396\n",
      "Na:  4396\n",
      "Ns:  4396\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "from pickle import load\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "from random import randrange\n",
    "from word_score import get_word_score\n",
    "from sentence_score import extractive_summarizer \n",
    "import string\n",
    "\n",
    "model.save('my-model.h5')\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "\twith open('dataset.pkl', 'rb') as fp:\n",
    "\t\tarticle_word_list, summary_word_list = load(fp)\n",
    "\n",
    "\tword_score_dictionary = get_word_score(article_word_list, summary_word_list)\n",
    "\tx, y = load_dataset()\n",
    "\tfixed_x = []\n",
    "\tfor i in range(len(x)):\n",
    "\t\tx[i].maketrans('','', string.punctuation)\n",
    "\t\tif(len(x[i].split()) > 150):\n",
    "\t\t\tans = \" \".join(e for e in x[i].split()[:100])\n",
    "\t\t\tfixed_x.append(ans)\n",
    "\t\telse:\n",
    "\t\t\ttry :\n",
    "\t\t\t\tsummary = extractive_summarizer(x[i], word_score_dictionary, sentence_count = 6)\n",
    "\t\t\texcept ZeroDivisionError :\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tz = summary.split()\n",
    "\t\t\tif(len(z)>100):\n",
    "\t\t\t\tans = \" \".join(e for e in z[:100])\n",
    "\t\t\t\tfixed_x.append(ans)\n",
    "\t\t\telse:\n",
    "\t\t\t\tpre_len = len(z)\n",
    "\t\t\t\tfor i in range(100 - pre_len):\n",
    "\t\t\t\t\tz.append(z[randrange(pre_len)])\n",
    "\t\t\t\tans = \" \".join(e for e in z)\t\n",
    "\t\t\t\tfixed_x.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_aXE0dK7LyiD"
   },
   "outputs": [],
   "source": [
    "#model.save(\"my_model.h5\")\n",
    "model = load_model(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "bLNRAFdNS-qP"
   },
   "outputs": [],
   "source": [
    "#!ls\n",
    "#files.download(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "A94OvyUZTnkG"
   },
   "outputs": [],
   "source": [
    "# def preprocess_input(article) :\n",
    "#   article = article.maketrans('', '', string.punctuation)\n",
    "#   article = \" \".join(article.split()[:100])\n",
    "#   vector = []\n",
    "#   for word in article.split() :\n",
    "#     vector.append(model_w2v[word])\n",
    "#   return vector\n",
    "# # \tif(len(x[i].split()) < 150):\n",
    "# #     ans = \" \".join(e for e in article.split()[:100])\n",
    "# # \telse:\n",
    "# # \t\t article = extractive_summarizer(article, word_score_dictionary, sentence_count = 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "EnM6Sn1SY7-B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lashkar-e-Taiba's Kashmir commander Abu Dujana was killed in an encounter in a village in Pulwama district of Jammu and Kashmir earlier this week. Dujana, who had managed to give the security forces a slip several times in the past, carried a bounty of Rs 15 lakh on his head.Reports say that Dujana had come to meet his wife when he was trapped inside a house in Hakripora village. Security officials involved in the encounter tried their best to convince Dujana to surrender but he refused, reports say.According to reports, Dujana rejected call for surrender from an Army officer. The Army\n"
     ]
    }
   ],
   "source": [
    "some_article = fixed_x[3]\n",
    "print(some_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lashkar-e-taibas kashmir commander abu dujana was killed in an encounter in a village in pulwama district of jammu and kashmir earlier this week dujana who had managed to give the security forces a slip several times in the past carried a bounty of rs 15 lakh on his head reports say that dujana had come to meet his wife when he was trapped inside a house in hakripora village security officials involved in the encounter tried their best to convince dujana to surrender but he refused reports say according to reports dujana rejected call for surrender from an army officer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Varun\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "C:\\Users\\Varun\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "def normalize_text(text, lim = 100) :\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"'\", \"\") #wouldn't -> wouldnt\n",
    "    for char in list(punctuation) : #remove all punctuation\n",
    "        if char != \"-\" and char != \"-\":\n",
    "            text = text.replace(char, \" \")\n",
    "    #text = \" \".join(filter(lambda x : x not in stopwords , text.split())) #remove stopwords\n",
    "    #text = \" \".join(filter(lambda x : x.isalpha(), text.split())) #remove numbers\n",
    "    #text = \" \".join([porter.stem(x) for x in text.split()]) #stemming\n",
    "    text = \" \".join(text.split()) #ensure words delimited by single space only\n",
    "    return \" \".join(text.split()[:lim])\n",
    "\n",
    "inp = []\n",
    "some_article = normalize_text(some_article)\n",
    "print(some_article)\n",
    "for word in some_article.split() :\n",
    "    if word not in model_w2v :\n",
    "        inp.append(np.array([random() for x in range(100)]))\n",
    "    else :\n",
    "        inp.append(model_w2v[word])\n",
    "inp = np.array(inp)\n",
    "# summary = []\n",
    "# print(len(model_w2v.vocab_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (60, 100) <class 'numpy.ndarray'> (100, 100)\n"
     ]
    }
   ],
   "source": [
    "arr = np.array([0 for x in range(100)])\n",
    "summary = np.array([dc(arr) for x in range(60)])\n",
    "print(type(summary), summary.shape, type(inp), inp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  1.30756554e-04   9.88543325e-05   1.37591109e-04 ...,   1.10135261e-04\n",
      "     1.03668528e-04   1.71137835e-05]\n",
      "  [  1.03160237e-04   6.57361816e-05   1.03010214e-04 ...,   7.73525462e-05\n",
      "     6.88751607e-05   5.84975851e-06]\n",
      "  [  1.03154489e-04   6.20565843e-05   9.96522940e-05 ...,   7.28034429e-05\n",
      "     6.42519080e-05   4.64172854e-06]\n",
      "  ..., \n",
      "  [  1.14317081e-04   5.54842845e-05   1.01215497e-04 ...,   6.70053196e-05\n",
      "     5.99363739e-05   3.26151803e-06]\n",
      "  [  1.14365066e-04   5.55173974e-05   1.01269805e-04 ...,   6.70268273e-05\n",
      "     5.99073501e-05   3.25785140e-06]\n",
      "  [  1.14412709e-04   5.55506485e-05   1.01323356e-04 ...,   6.70485897e-05\n",
      "     5.98794941e-05   3.25429414e-06]]]\n",
      "(1, 60, 4497)\n"
     ]
    }
   ],
   "source": [
    "k = model.predict([np.array([inp]), np.array([summary])])\n",
    "print(k)\n",
    "print(k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Varun\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "C:\\Users\\Varun\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$100,000', '$100,000', '$2', '$2', '$2', '$2', '$2', 'Budget', 'Budget', 'Budget', 'Budget', 'Budget', 'Budget', 'Budget', 'Budget', 'Budget', 'Budget', 'Budget', 'Budget', 'Budget', 'Budget', 'Budget', 'Budget', 'Budget', 'Budget', 'Budget', 'Budget', 'Budget', 'Budget', 'Budget', 'Budget', 'Budget', 'Budget', 'Davis', 'Davis', 'Davis', 'Davis', 'Davis', 'Davis', 'Hillary', 'Hillary', 'Hillary', 'Hillary', 'Hillary', 'Hillary', 'Hillary', 'Hillary', 'Hillary', 'Hillary', 'Hillary', 'Hillary', 'Hillary', 'Hillary', 'Has', 'Has', 'Has', 'Has', 'Has', 'Has', 'Has']\n"
     ]
    }
   ],
   "source": [
    "def create_summary_vec(summary_so_far = []) :\n",
    "    unk = model_w2v[\"__unk__\"]\n",
    "    return np.array([unk for x in range(60)])\n",
    "\n",
    "current_summary= create_summary_vec()\n",
    "summary_text = []\n",
    "for i in range(60) :\n",
    "    k = model.predict([np.array([inp]), np.array([current_summary])])\n",
    "    inverted = label_encoder.inverse_transform([np.argmax(i) for i in k[0]])\n",
    "    try :\n",
    "        current_summary[i] = model_w2v[inverted[i]]\n",
    "        summary_text.append(inverted[i])\n",
    "    except :\n",
    "        current_summary[i] = model_w2v[\"__unk__\"]\n",
    "print(summary_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Varun\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected input_2 to have shape (60, 100) but got array with shape (1, 100)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-5934c33ea790>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_w2v\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minverted\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0minverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1815\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[0;32m   1816\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1817\u001b[1;33m                                     check_batch_axis=False)\n\u001b[0m\u001b[0;32m   1818\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1819\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    121\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    124\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking : expected input_2 to have shape (60, 100) but got array with shape (1, 100)"
     ]
    }
   ],
   "source": [
    "inverted = label_encoder.inverse_transform(current_summary)\n",
    "print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Trial2.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
