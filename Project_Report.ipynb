{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                        Text summarisation\n",
    "\n",
    "\n",
    "There are two prominent types of summarization algorithms.\n",
    "\n",
    "1 Extractive Summarization.\n",
    "\n",
    "2 Abstractive Summarization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1 Extractive Summarization \n",
    "\n",
    "\n",
    "Extractive summarization techniques produce summaries by choosing a subset of the sentences in the original text. These summaries contain the most important sentences of the input. Input can be a single document or multiple documents. In order to better understand how summarization systems work, we describe three fairly independent tasks which all summarizers perform:\n",
    "\n",
    "\n",
    "1) Construct an intermediate representation of the input text which expresses the main aspects of the text.\n",
    "\n",
    "2) Score the sentences based on the representation.\n",
    "\n",
    "3) Select a summary comprising of a number of sentences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Abstractive Summarization\n",
    "\n",
    "Abstractive summarization methods aim at producing important material in a new way. In other words, they interpret and examine the text using advanced natural language techniques in order to generate a new shorter text that conveys the most critical information from the original text. Existing abstractive summarizers often rely on an extractive preprocessing component to produce the abstract of the text. \n",
    "\n",
    "We use an Encoder-Decoder architecture for our problem. The Encoder-Decoder architecture with attention is popular for a suite of natural language processing problems that generate variable length output sequences, such as text summarization.\n",
    "\n",
    "The application of architecture to text summarization is as follows:\n",
    "Encoder: The encoder is responsible for reading the source document and encoding it to an internal representation.\n",
    "Decoder: The decoder is a language model responsible for generating each word in the output summary using the encoded representation of the source document.\n",
    "\n",
    "The Encoder generates a context vector representation of the source document. This document is fed to the decoder at each step of the generated output sequence. This allows the decoder to build up the same internal state as was used to generate the words in the output sequence so that it is primed to generate the next word in the sequence.\n",
    "\n",
    "This process is then repeated by calling the model again and again for each word in the output sequence until a maximum length or end-of-sequence token is generated. In our case, we fix the output sequence to be 60 words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a)   Architectural Design\n",
    "    1  Extractive Summariser : \n",
    "    \n",
    "        Pre computed table of word scores for every word in the dataset.\n",
    "\n",
    "    2  Abstractive Summariser : Multilayer Encoder-Decoder Recurrent Neural Network with LSTM units.\n",
    "\n",
    "<img src=\"image.png\">\n",
    "                            \n",
    "\n",
    "b)   Detailed Design- Pseudo code/Algorithm\n",
    "\n",
    "   1 Score words and sentences based on probabilities based on occurrence in ctext and text.\n",
    "\n",
    "   2 Normalise scores using smoothing algorithms.\n",
    "  \n",
    "   3 Develop extractive summarisation table using the scores. Use this as benchmark for abstractive summarisation.\n",
    "  \n",
    "   4 Create Multilayer Attentional Encoder-Decoder Recurrent Neural Network with LSTM units.\n",
    "   \n",
    "   5 Represent vocabulary with one hot vectors.\n",
    "   \n",
    "   6 Choose appropriate word embeddings for the network. \n",
    "   \n",
    "   7 Try various hyper parameters for the network and report results.\n",
    "\n",
    "\n",
    "                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture of the Encoder-Decoder architecture for text summarization in the Keras deep learning library.\n",
    "<img src=\"image2.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
