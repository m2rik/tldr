{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Varun\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Stories 92579\n",
      "they are the worlds cultural capitals the nerve centers of innovation and the engine rooms of economic growth but could cities also hold the key to cutting carbon emissions longterm a study from the world bank found that the largest cities and urban areas on the planet are now home to roughly million people and spew out some billion tons of greenhouse gasses every year as urban migration continues apace these figures are only expected to rise in the short term while this may initially lead to more pollutants being pumped into the earths atmosphere some experts believe it could work out better in the long term they say that the ecological efficiencies cities can offer aligned with their financial and political influence could lead to the development of more effective ways to curb carbon emissions as the worlds leading environmental figures gather in durban south africa for the united nations conference on climate change cnn asked two urban climate change experts to explain the complex role of cities dr stephen hammer is codirector of the urban climate change research network a consortium of academics and institutions dedicated to the analysis of climate change mitigation and an adviser to new york citys energy policy taskforce mike hodson meanwhile is a senior research fellow at the centre for sustainable urban and regional futures at the university of salford and coauthor of the book world cities and climate change how much do cities contribute to climate change stephen hammer sh cities are the where the majority of global energy use occurs by far the irony is however that the dense nature of cities can actually reduce the level of carbon emissions by introducing different kinds of efficiencies the sheer number of people however just means that you just end up with a large volume of energy use and emissions mike hodson mh cities are increasingly being characterized as significant producers of climate change just over half the worlds population lives in cities around threequarters of global energy consumption is linked to cities and around fourfifths of global greenhouse gas emissions are linked to cities in what ways can cities help to address the issue of climate change sh cities are often the laboratories for central government policies central governments dont often create these things on their own theyre looking at what others have done including subnational governments and saying well if it worked there we can make it work nationally therefore in the absence of nationallevel action it is possible for cities to take very concrete steps to influence overall emission levels mh the biggest cities are pretty powerful in terms of positions within their national economies theyve got pretty welldeveloped government structures theyve got mayors and related agencies but not only have they got those sorts of resources and therefore the ability to lobby and influence central government they also encompass quite significant national resources whether its financial centers centers of business and centers of media given that theyve got that range of expertise knowledge social networks and financial resources they can start to paint that picture that they are the places that can actively and effectively start to build climate change strategies and deliver on them why is it in cities interest to act in a way that negates the impact of climate change sh i think its very safe to say that climate change threatens the longterm economic viability of many cities in addition to creating public health risks hurricane katrina in new orleans is a great example of that although not an event that was necessarily caused by climate change the city suffered hugely in terms of the economic impact of an extreme weather event and these types of events are assumed to become more commonplace as the climate changes mh i think the flipside of this sort of argument about cities being producers of climate change is that theyre also increasingly being seen as victims of climate change this is particularly the case with rising sea levels coastal cities and riverside cities that are at risk from rising sea levels but also those susceptible to drought or urban heat islands what can cities do to negate or prepare for the impact of climate change sh it becomes particularly important for cities as they expand rapidly to make the decisions today that will constrain emissions in the future so again going back to some of the first things i was talking about the way the city is designed having it so that it promotes density that that then supports public transportation ridership designing the city in a way that makes it bicyclefriendly or ecofriendly or pedestrian friendly so youre not always forcing people into automobiles you must make the right decisions right now and as the city expands going forward you must constantly revisit them to see how can we be changing the old city to be more efficient but also how it can maintain efficiency when we are designing the expanding city or the new city mh one of the things that strikes me is that whether its global cities or more ordinary cities to different degrees they have started to get their strategic act together by developing strategies setting targets setting timelines but as of yet theyve not managed to translate that into any sort of effective way theyve really got to get to the more practical elements of how to translate that into tangible actions and deliver on them\n",
      "world bank cities produce roughly billion tons of carbon emissions each year some scientists believe expanding cities could help curb carbon emissions in the long term two urban climate change experts give their views on cities importance to curbing future emissions\n",
      "count    92579.000000\n",
      "mean       639.287862\n",
      "std        336.105693\n",
      "min          0.000000\n",
      "25%        372.000000\n",
      "50%        590.000000\n",
      "75%        852.000000\n",
      "max       1821.000000\n",
      "dtype: float64\n",
      "count    92579.000000\n",
      "mean        40.959051\n",
      "std          9.504003\n",
      "min          7.000000\n",
      "25%         34.000000\n",
      "50%         41.000000\n",
      "75%         48.000000\n",
      "max        104.000000\n",
      "dtype: float64\n",
      "Size of vocabulary :  324216\n",
      "92579 92579\n",
      "5000 5000\n",
      "{100}\n",
      "{30}\n",
      "130\n",
      "['driving', 'in', 'pakistans', 'swat', 'valley', 'a', 'region', 'located', 'close', 'to', 'the', 'border', 'with', 'afghanistan', 'i', 'can', 'sense', 'the', 'difference', 'immediately', 'the', 'faces', 'of', 'young', 'children', 'women', 'and', 'families', 'who', 'walk', 'around', 'seem', 'relaxed', 'the', 'streets', 'buzz', 'with', 'an', 'air', 'of', 'calm', 'swat', 'is', 'full', 'of', 'life', 'once', 'again', 'its', 'a', 'marked', 'difference', 'from', 'the', 'previous', 'times', 'ive', 'been', 'here', 'the', 'last', 'time', 'was', 'shortly', 'after', 'malala', 'yousafzai', 'was', 'shot', 'back', 'then', 'a', 'sense', 'of', 'sorrow', 'and', 'shock', 'hung', 'in', 'the', 'air', 'everything', 'felt', 'still', 'and', 'drained', 'the', 'town', 'was', 'in', 'mourning', 'in', 'these', 'past', 'nine', 'months', 'we', 'have', 'written', 'and', 'cnns', 'saima', 'mohsin', 'returns', 'to', 'swat', 'valley', 'where', 'malala', 'was', 'shot', 'female', 'students', 'at', 'school', 'embrace', 'education', 'despite', 'threats', 'and', 'fears', 'malalas', 'town', 'is', 'recovering', 'with', 'students', 'returning', 'to', 'school']\n",
      "['driving', 'in', 'pakistans', 'swat', 'valley', 'a', 'region', 'located', 'close', 'to', 'the', 'border', 'with', 'afghanistan', 'i', 'can', 'sense', 'the', 'difference', 'immediately', 'the', 'faces', 'of', 'young', 'children', 'women', 'and', 'families', 'who', 'walk', 'around', 'seem', 'relaxed', 'the', 'streets', 'buzz', 'with', 'an', 'air', 'of', 'calm', 'swat', 'is', 'full', 'of', 'life', 'once', 'again', 'its', 'a', 'marked', 'difference', 'from', 'the', 'previous', 'times', 'ive', 'been', 'here', 'the', 'last', 'time', 'was', 'shortly', 'after', 'malala', 'yousafzai', 'was', 'shot', 'back', 'then', 'a', 'sense', 'of', 'sorrow', 'and', 'shock', 'hung', 'in', 'the', 'air', 'everything', 'felt', 'still', 'and', 'drained', 'the', 'town', 'was', 'in', 'mourning', 'in', 'these', 'past', 'nine', 'months', 'we', 'have', 'written', 'and', 'cnns', 'saima', 'mohsin', 'returns', 'to', 'swat', 'valley', 'where', 'malala', 'was', 'shot', 'female', 'students', 'at', 'school', 'embrace', 'education', 'despite', 'threats', 'and', 'fears', 'malalas', 'town', 'is', 'recovering', 'with', 'students', 'returning', 'to', 'school']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New vocab size 10816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Varun\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:113: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\Varun\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:114: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 100, 100)\n",
      "(5000, 30, 100)\n",
      "['driving', 'in', 'pakistans', 'swat', 'valley', 'a', 'region', 'located', 'close', 'to']\n",
      "[10695]\n",
      "(5000, 30, 10816)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "#Load dataset from pickled file \n",
    "\n",
    "from pickle import load\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import TimeDistributed, RepeatVector, LSTM, Dense, Embedding, Input, concatenate\n",
    "from keras.preprocessing.text import Tokenizer, one_hot, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import Word2Vec\n",
    "from collections import Counter\n",
    "from random import randrange\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "considered_size = 5000 #increase this on better machine\n",
    "\n",
    "stories = load(open('cnn_dataset.pkl', 'rb'))\n",
    "print('Loaded Stories %d' % len(stories))\n",
    "\n",
    "\n",
    "articles = []\n",
    "summaries = []\n",
    "for story in stories :\n",
    "    article = story['story']\n",
    "    new_article = \"\"\n",
    "    for item in article :\n",
    "        new_article += item + \" \"\n",
    "    new_article = \" \".join(new_article.split())\n",
    "    articles.append(new_article)\n",
    "    \n",
    "    summary = story['highlights']\n",
    "    new_summary = \"\"\n",
    "    for item in summary :\n",
    "        new_summary += item + \" \"\n",
    "    new_summary = \" \".join(new_summary.split())\n",
    "    summaries.append(new_summary)\n",
    "print(articles[5], summaries[5], sep = \"\\n\")\n",
    "del stories\n",
    "\n",
    "article_lengths = []\n",
    "summary_lengths = []\n",
    "for article in articles :\n",
    "    article_lengths.append(len(article.split()))\n",
    "for summary in summaries :\n",
    "    summary_lengths.append(len(summary.split()))\n",
    "\n",
    "\n",
    "\n",
    "a = pd.Series(article_lengths)\n",
    "b = pd.Series(summary_lengths)\n",
    "print(a.describe())\n",
    "print(b.describe())\n",
    "del a, b, article_lengths, summary_lengths\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "all_words = \" \".join(articles) + \" \".join(summaries)\n",
    "all_words = text_to_word_sequence(all_words)\n",
    "vocab_size = len(set(all_words)) #56558, 27178 for only summaries with 60 words\n",
    "\n",
    "print(\"Size of vocabulary : \", vocab_size)\n",
    "del all_words\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "print(len(articles), len(summaries))\n",
    "articles = articles[:considered_size]\n",
    "summaries = summaries[:considered_size]\n",
    "print(len(articles), len(summaries))\n",
    "\n",
    "# tokenized x, y and xy\n",
    "\n",
    "tok_x = [text_to_word_sequence(xi)[:100] + max(0, 100 - len(text_to_word_sequence(xi)[:100])) * [\"__unk__\"] for xi in articles]\n",
    "tok_y = [text_to_word_sequence(yi)[:30] + max(0, 30 - len(text_to_word_sequence(yi)[:30])) * [\"__unk__\"] for yi in summaries]\n",
    "tok_xy = [xi + yi for xi, yi in zip(tok_x, tok_y)]\n",
    "\n",
    "len_tok_x = set([len(xi) for xi in tok_x])\n",
    "print(len_tok_x)\n",
    "\n",
    "len_tok_y = set([len(yi) for yi in tok_y])\n",
    "print(len_tok_y)\n",
    "\n",
    "print(len(tok_xy[0]))\n",
    "print(tok_xy[0])\n",
    "\n",
    "tok_xy.append(5 * [\"__unk__\"])\n",
    "print(tok_xy[0])\n",
    "\n",
    "# word2vec model\n",
    "model_w2v = Word2Vec(tok_xy, size = 100, min_count = 5)\n",
    "\n",
    "# size of vocab\n",
    "words = list(model_w2v.wv.vocab)\n",
    "vocab_size = len(words)\n",
    "print(\"New vocab size\",vocab_size)\n",
    "\n",
    "# replacing unknown words\n",
    "tok_x = [[word if word in model_w2v.wv.vocab else \"__unk__\" for word in xi] for xi in tok_x]\n",
    "tok_y = [[word if word in model_w2v.wv.vocab else \"__unk__\" for word in yi] for yi in tok_y]\n",
    "\n",
    "# x values\n",
    "x_train_article = np.array([[model_w2v[word] for word in xi] for xi in tok_x])\n",
    "x_train_summary = np.array([[model_w2v[word] for word in yi] for yi in tok_y])\n",
    "\n",
    "print(x_train_article.shape)\n",
    "print(x_train_summary.shape)\n",
    "\n",
    "\n",
    "del tok_x\n",
    "del tok_xy\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "print(words[:10])\n",
    "\n",
    "# integer encode\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(words)\n",
    "\n",
    "del words\n",
    "\n",
    "print(label_encoder.transform([\"world\"]))\n",
    "\n",
    "# one-hot encode\n",
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(range(vocab_size))\n",
    "\n",
    "y_train = np.array([label_binarizer.transform(label_encoder.transform(yi)) for yi in tok_y])\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input arrays should have the same number of samples as target arrays. Found 5000 input samples and 30 target samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-ac4d9fa693e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx_train_article\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train_summary\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1628\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1629\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1630\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1631\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m   1488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1489\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_array_lengths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1490\u001b[1;33m             \u001b[0m_check_array_lengths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1491\u001b[0m         _check_loss_and_target_compatibility(y,\n\u001b[0;32m   1492\u001b[0m                                              \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed_loss_fns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_check_array_lengths\u001b[1;34m(inputs, targets, weights)\u001b[0m\n\u001b[0;32m    218\u001b[0m                          \u001b[1;34m'the same number of samples as target arrays. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                          \u001b[1;34m'Found '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' input samples '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[0;32m    221\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m         raise ValueError('All sample_weight arrays should have '\n",
      "\u001b[1;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 5000 input samples and 30 target samples."
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "src_txt_length = 100\n",
    "sum_txt_length = 30\n",
    "word_vec_length = 100\n",
    "\n",
    "# article input model\n",
    "inputs1 = Input(shape=(src_txt_length, word_vec_length))\n",
    "# article1 = Embedding(vocab_size, 128)(inputs1)\n",
    "article2 = LSTM(128)(inputs1)\n",
    "article3 = RepeatVector(sum_txt_length)(article2)\n",
    "\n",
    "# summary input model\n",
    "inputs2 = Input(shape=(sum_txt_length, word_vec_length))\n",
    "# summ1 = Embedding(vocab_size, 128)(inputs2)\n",
    "\n",
    "# decoder model\n",
    "decoder1 = concatenate([article3, inputs2])\n",
    "decoder2 = LSTM(128)(decoder1)                           # made changes here\n",
    "outputs = Dense(vocab_size, activation='softmax')(decoder2)      # made changes here\n",
    "\n",
    "# tie it together [article, summary] [word]\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['acc'])\n",
    "model.fit([x_train_article, x_train_summary], y_train, epochs = epochs, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5000/5000 [==============================] - 358s 72ms/step - loss: 7.3498\n",
      "Epoch 2/10\n",
      "5000/5000 [==============================] - 350s 70ms/step - loss: 7.0523\n",
      "Epoch 3/10\n",
      "5000/5000 [==============================] - 353s 71ms/step - loss: 7.0507\n",
      "Epoch 4/10\n",
      "5000/5000 [==============================] - 357s 71ms/step - loss: 7.0504\n",
      "Epoch 5/10\n",
      "5000/5000 [==============================] - 378s 76ms/step - loss: 7.0500\n",
      "Epoch 6/10\n",
      "5000/5000 [==============================] - 360s 72ms/step - loss: 7.0512\n",
      "Epoch 7/10\n",
      "5000/5000 [==============================] - 361s 72ms/step - loss: 7.0507\n",
      "Epoch 8/10\n",
      "5000/5000 [==============================] - 355s 71ms/step - loss: 7.0508\n",
      "Epoch 9/10\n",
      "5000/5000 [==============================] - 363s 73ms/step - loss: 7.0504\n",
      "Epoch 10/10\n",
      "5000/5000 [==============================] - 352s 70ms/step - loss: 7.0503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x145b13deda0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vocab_size = ...\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "src_txt_length = 100\n",
    "sum_txt_length = 30\n",
    "word_vec_length = 100\n",
    "# encoder input model\n",
    "inputs = Input(shape=(src_txt_length, word_vec_length))\n",
    "#encoder1 = Embedding(vocab_size, 128)(inputs)\n",
    "encoder2 = LSTM(128)(inputs)\n",
    "encoder3 = RepeatVector(sum_txt_length)(encoder2)\n",
    "# decoder output model\n",
    "decoder1 = LSTM(128, return_sequences=True)(encoder3)\n",
    "outputs = TimeDistributed(Dense(vocab_size, activation='softmax'))(decoder1)\n",
    "# tie it together\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(x_train_article, y_train, epochs = epochs, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  2.65412312e-02   8.76063388e-03   8.91163218e-05 ...,   8.81302185e-05\n",
      "     1.85262998e-05   4.66172387e-05]\n",
      "  [  7.77303204e-02   1.91469472e-02   5.80466731e-05 ...,   5.77768515e-05\n",
      "     8.02972227e-06   2.58804430e-05]\n",
      "  [  8.95150155e-02   2.10824180e-02   5.30885409e-05 ...,   5.28941018e-05\n",
      "     6.89927629e-06   2.30971345e-05]\n",
      "  ..., \n",
      "  [  9.14949030e-02   2.13951990e-02   5.23056951e-05 ...,   5.21224028e-05\n",
      "     6.73022896e-06   2.26679131e-05]\n",
      "  [  9.14949030e-02   2.13951990e-02   5.23056951e-05 ...,   5.21224028e-05\n",
      "     6.73022896e-06   2.26679131e-05]\n",
      "  [  9.14949030e-02   2.13951990e-02   5.23056951e-05 ...,   5.21224028e-05\n",
      "     6.73022896e-06   2.26679131e-05]]]\n"
     ]
    }
   ],
   "source": [
    "a = model.predict(np.array([x_train_article[0]]))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__unk__' '__unk__' '__unk__' '__unk__' '__unk__' '__unk__' '__unk__'\n",
      " '__unk__' '__unk__' '__unk__' '__unk__' '__unk__' '__unk__' '__unk__'\n",
      " '__unk__' '__unk__' '__unk__' '__unk__' '__unk__' '__unk__' '__unk__'\n",
      " '__unk__' '__unk__' '__unk__' '__unk__' '__unk__' '__unk__' '__unk__'\n",
      " '__unk__' '__unk__']\n"
     ]
    }
   ],
   "source": [
    "#k = model.predict(np.array([inp]))\n",
    "inverted = label_encoder.inverse_transform([np.argmax(i) for i in a[0]])\n",
    "print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
